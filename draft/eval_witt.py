import torch
import numpy as np
import pickle
from tqdm import tqdm
from sklearn.metrics import ndcg_score
from sparsecl.models import our_BertForCL
from transformers import AutoTokenizer, AutoConfig


def calculate_ndcg(q_vecs, d_vecs, qrels, k=10):
    """ è®¡ç®— NDCG@10 """
    scores = []
    for q_id, query_vec in enumerate(q_vecs):
        # è®¡ç®—è¯¥ Query å¯¹æ‰€æœ‰å€™é€‰æ–‡æ¡£çš„ç›¸ä¼¼åº¦
        sim_scores = torch.matmul(d_vecs, query_vec.unsqueeze(1)).squeeze().cpu().numpy()

        # æ„å»ºçœŸå€¼æ ‡ç­¾ (Ground Truth)
        # qrels[q_id] åº”è¯¥æ˜¯ä¸€ä¸ªæ•°ç»„ï¼ŒçŸ›ç›¾æ–‡æ¡£ä½ç½®ä¸º 1ï¼Œå…¶ä½™ä¸º 0
        labels = qrels[q_id]
        scores.append(ndcg_score([labels], [sim_scores], k=k))
    return np.mean(scores)


@torch.no_grad()
def evaluate_witt_model(model_path, data_path, device="cuda", alpha=1.0):
    # 1. åŠ è½½æ¨¡å‹ä¸é…ç½®
    config = AutoConfig.from_pretrained(model_path)
    if not hasattr(config, 'do_mlm'):
        config.do_mlm = False
    if not hasattr(config, 'temp'):
        config.temp = 0.05
    if not hasattr(config, 'mlp_only_train'):
        config.mlp_only_train = False
    config.pooler_type = "avg"
    tokenizer = AutoTokenizer.from_pretrained(model_path)
    model = our_BertForCL.from_pretrained(model_path, config=config, model_args=config)
    model.to(device).eval()

    # 2. åŠ è½½æ•°æ® (å‡è®¾ä½ å·²ç»æœ‰å¤„ç†å¥½çš„ Arguana pickle)
    with open(data_path, "rb") as f:
        # è¿™é‡Œçš„åŠ è½½é€»è¾‘éœ€å¯¹åº”ä½ æ•°æ®è„šæœ¬çš„ dump é¡ºåº
        corpus_data = pickle.load(f)  # {id: text}
        queries_data = pickle.load(f)  # {id: text}
        qrels_data = pickle.load(f)  # {q_id: {d_id: 1}}

    # 3. æå–ç‰¹å¾åµŒå…¥ (Embedding Extraction)
    print(">>> Extracting Embeddings...")
    q_ids = list(queries_data.keys())
    d_ids = list(corpus_data.keys())

    # å­˜å‚¨ a å‘é‡å’Œ b å‘é‡
    q_a, q_b = [], []
    d_a, d_b = [], []

    for i, q_id in enumerate(tqdm(q_ids, desc="Encoding Queries")):
        inputs = tokenizer(queries_data[q_id], return_tensors="pt", truncation=True, max_length=512).to(device)
        out = model(**inputs, sent_emb=True)  # è°ƒç”¨ä½ çš„ sentemb_forward

        # [ğŸ” CHECK] æ‰“å°å‰ 5 ä¸ªæ ·æœ¬çš„æ¨¡é•¿
        if i < 5:
            # è®¡ç®— L2 Norm
            norm_a = torch.norm(out["content"], p=2, dim=-1).item()
            norm_b = torch.norm(out["stance"], p=2, dim=-1).item()
            
            # ç†è®ºæœ€å¤§æ¨¡é•¿ (å¯¹äº Tanh æ¿€æ´»)
            # content_dim=768 -> sqrt(768) â‰ˆ 27.7
            # stance_dim=128  -> sqrt(128) â‰ˆ 11.3
            print(f"\n[Debug Sample {i}]")
            print(f"  Norm(a) [Content]: {norm_a:.4f}")
            print(f"  Norm(b) [Stance] : {norm_b:.4f}  <-- å…³æ³¨è¿™ä¸ªï¼å¦‚æœæ¥è¿‘ 11.3 è¯´æ˜é¥±å’Œäº†")
            
        q_a.append(torch.nn.functional.normalize(out["content"], p=2, dim=-1))
        q_b.append(torch.nn.functional.normalize(out["stance"], p=2, dim=-1))

    for d_id in tqdm(d_ids, desc="Encoding Corpus"):
        raw_input = corpus_data[d_id]

        # è‡ªåŠ¨å¤„ç†å­—å…¸ç±»å‹ (Arguana Corpus é€šå¸¸åŒ…å« title å’Œ text)
        if isinstance(raw_input, dict):
            # å°†æ ‡é¢˜å’Œæ­£æ–‡æ‹¼æ¥ï¼Œè¿™æ˜¯æ£€ç´¢ä»»åŠ¡çš„æ ‡å‡†åšæ³•
            title = raw_input.get("title", "")
            body = raw_input.get("text", "")
            text = (title + " " + body).strip()
        else:
            text = str(raw_input)

        # å¢åŠ ä¸€ä¸ªç©ºå€¼ä¿æŠ¤
        if not text:
            text = "empty document"

        inputs = tokenizer(text, return_tensors="pt", truncation=True, max_length=512).to(device)
        out = model(**inputs, sent_emb=True)
        d_a.append(torch.nn.functional.normalize(out["content"], p=2, dim=-1))
        d_b.append(torch.nn.functional.normalize(out["stance"], p=2, dim=-1))


    Q_A, Q_B = torch.cat(q_a), torch.cat(q_b)
    D_A, D_B = torch.cat(d_a), torch.cat(d_b)

    # 4. æ„é€  Label çŸ©é˜µ
    labels_matrix = np.zeros((len(q_ids), len(d_ids)))
    for i, q_id in enumerate(q_ids):
        for d_id, rel in qrels_data[q_id].items():
            if d_id in d_ids:
                labels_matrix[i, d_ids.index(d_id)] = rel

    # 5. æ ¸å¿ƒè¯„æµ‹é€»è¾‘
    print("\n" + "=" * 30)
    print("ğŸ† WITT DECOUPLING EVALUATION")
    print("=" * 30)

    # A. Content-Only (ä»…é å†…å®¹)
    ndcg_a = calculate_ndcg(Q_A, D_A, labels_matrix)
    print(f"ğŸ”¹ Content-Only NDCG@10 (a): {ndcg_a:.4f}")

    # B. Stance-Only (ä»…é ç«‹åœº)
    ndcg_b = calculate_ndcg(Q_B, D_B, labels_matrix)
    print(f"ğŸ”¹ Stance-Only  NDCG@10 (b): {ndcg_b:.4f}")

    # C. Witt-Decoupled (çŸ›ç›¾æ£€ç´¢å…¬å¼: Content - alpha * Stance)
    # é€»è¾‘ï¼šæˆ‘ä»¬è¦æ‰¾è¯é¢˜ç›¸åŒ (Aé«˜) ä½†ç«‹åœºç›¸å (Bä½) çš„æ–‡æ¡£
    combined_scores = []
    for i in range(len(q_ids)):
        score_a = torch.matmul(D_A, Q_A[i].unsqueeze(1)).squeeze()
        score_b = torch.matmul(D_B, Q_B[i].unsqueeze(1)).squeeze()
        # æ ¸å¿ƒï¼šå¯»æ‰¾ B ç›¸ä¼¼åº¦æœ€ä½çš„ä½œä¸ºçŸ›ç›¾
        final_score = score_a - alpha * score_b
        combined_scores.append(ndcg_score([labels_matrix[i]], [final_score.cpu().numpy()], k=10))

    print(f"ğŸ”¥ Witt-Combined NDCG@10 (a - {alpha}b): {np.mean(combined_scores):.4f}")
    print("=" * 30)


if __name__ == "__main__":
    # ä¿®æ”¹ä¸ºä½ çš„æ¨¡å‹è·¯å¾„å’Œæ•°æ®è·¯å¾„
    evaluate_witt_model(
        model_path="../results/repository/phase2/202512230039/our-bge-arguana-qr-phase2",
        data_path="../data/arguana_test_retrieval_final.pkl",
        alpha=0.05  # è¿™ä¸ªå‚æ•°å¯ä»¥åŠ¨æ€è°ƒä¼˜
    )
